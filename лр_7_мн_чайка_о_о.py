# -*- coding: utf-8 -*-
"""ЛР_7_МН_Чайка_О_О

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1MJBAG-U2j8DSlQo3BnTP9bNJUF-bkiP3

Чайка О.О. був присутнім на парі
"""

import matplotlib.pyplot as plt
from sklearn.datasets import load_iris
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score
from sklearn.preprocessing import StandardScaler
import pandas as pd

iris = load_iris()
x = iris.data

pd.DataFrame(x, columns=iris.feature_names).head()

# Масштабування даних

scaler = StandardScaler()

X_scaled = scaler.fit_transform(x)



# Вибір даних для кластеризації (ігноруємо перший стовпець)

X_2d = pd.DataFrame(X_scaled, columns=iris.feature_names).iloc[:, 1:]  # Вибираємо всі рядки і всі стовпці, крім першого



# Застосування ліктевого методу для визначення оптимальної кількості кластерів

wcss_2d = []

for i in range(1, 11):

    kmeans = KMeans(n_clusters=i, init='k-means++', max_iter=300, n_init=10, random_state=0)

    kmeans.fit(X_2d)

    wcss_2d.append(kmeans.inertia_)



# Візуалізація ліктевого графіка

plt.figure(figsize=(10, 6))

plt.plot(range(1, 11), wcss_2d, marker='o', linestyle='--')

plt.title('Elbow Method for Iris Dataset after Scaling')

plt.xlabel('Number of clusters')

plt.ylabel('Within-cluster Sum of Squares (WCSS)')

plt.grid(True)

plt.show()

# Застосування методу силуетів для визначення оптимальної кількості кластерів
silhouette_scores = []
for i in range(2, 11):  # Для методу силуетів потрібно мінімум 2 кластери
    kmeans = KMeans(n_clusters=i, init='k-means++', max_iter=300, n_init=10, random_state=0)
    cluster_labels = kmeans.fit_predict(X_2d)
    silhouette_avg = silhouette_score(X_2d, cluster_labels)
    silhouette_scores.append(silhouette_avg)

# Візуалізація графіка методу силуетів
plt.figure(figsize=(10, 6))
plt.plot(range(2, 11), silhouette_scores, marker='o', linestyle='--')
plt.title('Silhouette Method for Iris Dataset after Scaling')
plt.xlabel('Number of clusters')
plt.ylabel('Silhouette Score')
plt.grid(True)
plt.show()

import seaborn as sns
# Використання KMeans для кластеризації
kmeans = KMeans(n_clusters=2, init='k-means++', max_iter=300, n_init=10, random_state=42)
clusters = kmeans.fit_predict(X_scaled)

# Візуалізація кластерів
plt.figure(figsize=(10, 6))
sns.scatterplot(x=X_scaled[:, 0], y=X_scaled[:, 1], hue=clusters, palette='viridis', legend='full')
plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], s=300, c='red', label='Centroids')
plt.title('KMeans Clustering of Iris Dataset')
plt.xlabel('Feature 1 (Scaled)')
plt.ylabel('Feature 2 (Scaled)')
plt.legend(title='Cluster')
plt.grid(True)
plt.show()

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score, classification_report

from google.colab import files

from google.colab import files
uploaded = files.upload()

import pandas as pd
df = pd.read_csv("Student_data.csv")

df.columns

df.info()

df.isnull().sum()

df.duplicated().sum()

df.describe()

X = df.drop('GradeClass', axis=1)
y = df['GradeClass']
X_train, X_test, y_train, y_test =train_test_split(X, y, test_size=0.2, random_state=42)
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

# Параметри для пошуку
param_grid_rf = {
    'n_estimators': [100, 200, 300],
    'max_depth': [None, 10, 20, 30],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4]
}

# Модель
rf = RandomForestClassifier(random_state=42)

# GridSearchCV
grid_search_rf = GridSearchCV(estimator=rf, param_grid=param_grid_rf, cv=5, n_jobs=-1, verbose=2)
grid_search_rf.fit(X_train, y_train)

# Найкращі параметри та точність
print("Best parameters for Random Forest:", grid_search_rf.best_params_)
print("Best cross-validation accuracy for Random Forest:", grid_search_rf.best_score_)

# Оцінка на тестовій вибірці
y_pred_rf = grid_search_rf.predict(X_test)

# Параметри для пошуку
param_grid_lr = {
    'penalty': ['l1', 'l2'],
    'C': [0.01, 0.1, 1, 10, 100],
    'solver': ['liblinear', 'saga']
}

# Модель
lr = LogisticRegression(random_state=42, max_iter=1000)

# GridSearchCV
grid_search_lr = GridSearchCV(estimator=lr, param_grid=param_grid_lr, cv=5, n_jobs=-1, verbose=2)
grid_search_lr.fit(X_train, y_train)

# Найкращі параметри та точність
print("Best parameters for Logistic Regression:", grid_search_lr.best_params_)
print("Best cross-validation accuracy for Logistic Regression:", grid_search_lr.best_score_)

# Оцінка на тестовій вибірці
y_pred_lr = grid_search_lr.predict(X_test)
print("Test accuracy for Logistic Regression:", accuracy_score(y_test, y_pred_lr))

# Параметри для пошуку
param_grid_svm = {
    'C': [0.1, 1, 10, 100],
    'gamma': [1, 0.1, 0.01, 0.001],
    'kernel': ['rbf', 'linear']
}

# Модель
svm = SVC(random_state=42)

# GridSearchCV
grid_search_svm = GridSearchCV(estimator=svm, param_grid=param_grid_svm, cv=5, n_jobs=-1, verbose=2)
grid_search_svm.fit(X_train, y_train)

# Найкращі параметри та точність
print("Best parameters for SVM:", grid_search_svm.best_params_)
print("Best cross-validation accuracy for SVM:", grid_search_svm.best_score_)

# Оцінка на тестовій вибірці
y_pred_svm = grid_search_svm.predict(X_test)
print("Test accuracy for SVM:", accuracy_score(y_test, y_pred_svm))

# Вибір числових стовпців для кластеризації
# Вибираємо стовпці, які є числовими і можуть впливати на кластеризацію
features = df[['Age', 'StudyTimeWeekly', 'Absences', 'GPA']]

# Масштабування даних
scaler = StandardScaler()
X_scaled = scaler.fit_transform(features)

# Застосування ліктевого методу для визначення оптимальної кількості кластерів
wcss = []
for i in range(1, 11):
    kmeans = KMeans(n_clusters=i, init='k-means++', max_iter=300, n_init=10, random_state=42)
    kmeans.fit(X_scaled)
    wcss.append(kmeans.inertia_)

# Візуалізація графіка ліктя
plt.figure(figsize=(10, 6))
plt.plot(range(1, 11), wcss, marker='o', linestyle='--')
plt.title('Elbow Method for Optimal Number of Clusters')
plt.xlabel('Number of clusters')
plt.ylabel('WCSS')
plt.grid(True)
plt.show()

# Застосування методу силуетів для визначення оптимальної кількості кластерів
silhouette_scores = []
for i in range(2, 11):  # Для методу силуетів потрібно мінімум 2 кластери
    kmeans = KMeans(n_clusters=i, init='k-means++', max_iter=300, n_init=10, random_state=42)
    cluster_labels = kmeans.fit_predict(X_scaled)
    silhouette_avg = silhouette_score(X_scaled, cluster_labels)
    silhouette_scores.append(silhouette_avg)

# Візуалізація графіка методу силуетів
plt.figure(figsize=(10, 6))
plt.plot(range(2, 11), silhouette_scores, marker='o', linestyle='--')
plt.title('Silhouette Method for Optimal Number of Clusters')
plt.xlabel('Number of clusters')
plt.ylabel('Average Silhouette Score')
plt.grid(True)
plt.show()

"""2 спосіб"""

!pip install kneed --quiet

from sklearn.decomposition import PCA
from kneed import KneeLocator
import numpy as np
from yellowbrick.cluster import KElbowVisualizer

X = df.drop(columns=['StudentID', 'GradeClass'])

X = StandardScaler().fit_transform(X)

pca = PCA(random_state=42).fit(X)
pve = pca.explained_variance_ratio_

sns.set_theme()

kneedle = KneeLocator(
    x=range(1, len(pve) + 1),
    y=pve,
    curve='convex',
    direction='decreasing'
)

# Візуалізація
kneedle.plot_knee()
plt.title(f'Knee Point at {kneedle.elbow + 1}')
plt.xlabel("Number of components")
plt.ylabel("Explained Variance Ratio")
plt.grid(True)
plt.show()

n_components = kneedle.elbow

ax = sns.lineplot(np.cumsum(pve))

ax.axvline(x=n_components,
     c='black',
     linestyle='--',
     linewidth=0.75)

ax.axhline(y=np.cumsum(pve)[n_components],
     c='black',
     linestyle='--',
     linewidth=0.75)

ax.set(xlabel='number of components',
   ylabel='cumulative explained variance')

plt.show()

model_kmn = KMeans(random_state=42)

visualizer = KElbowVisualizer(
    model_kmn,
    k=(2, 10),
    timings=False)

visualizer.fit(X)
visualizer.show()